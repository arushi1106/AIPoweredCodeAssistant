{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:04:41.813819Z",
     "start_time": "2025-05-10T23:04:40.607721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast"
   ],
   "id": "830edce7a9f9f142",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:04:44.198801Z",
     "start_time": "2025-05-10T23:04:42.737953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CSV with engineered features\n",
    "df = pd.read_csv(\"datasets/security/train_preprocessed.csv\")"
   ],
   "id": "14a20d1fc03420b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:05:10.805843Z",
     "start_time": "2025-05-10T23:04:44.635707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert string lists back to real lists\n",
    "df[\"input_ids\"] = df[\"input_ids\"].apply(ast.literal_eval)\n",
    "df[\"attention_mask\"] = df[\"attention_mask\"].apply(ast.literal_eval)\n",
    "df[\"target\"] = df[\"target\"].astype(int)"
   ],
   "id": "5df6c1cb4dd4f4b6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:05:13.112199Z",
     "start_time": "2025-05-10T23:05:10.811370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"target\"], random_state=42)"
   ],
   "id": "5b3c9e1089d330e4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:05:21.607968Z",
     "start_time": "2025-05-10T23:05:21.602462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CodeTokenDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df[\"input_ids\"].tolist()\n",
    "        self.masks = df[\"attention_mask\"].tolist()\n",
    "        self.labels = df[\"target\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            torch.tensor(self.masks[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )"
   ],
   "id": "f281027cc67176e9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:05:22.957363Z",
     "start_time": "2025-05-10T23:05:22.947932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_fn(batch):\n",
    "    input_ids, attention_masks, labels = zip(*batch)\n",
    "\n",
    "    input_ids = [x.clone().detach() if isinstance(x, torch.Tensor) else torch.tensor(x, dtype=torch.long) for x in input_ids]\n",
    "    attention_masks = [x.clone().detach() if isinstance(x, torch.Tensor) else torch.tensor(x, dtype=torch.long) for x in attention_masks]\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ],
   "id": "c2cea9592d9c03c3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:05:23.871685Z",
     "start_time": "2025-05-10T23:05:23.864165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = CodeTokenDataset(train_df)\n",
    "val_dataset = CodeTokenDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)"
   ],
   "id": "21d3796ff332329a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T02:20:49.473396Z",
     "start_time": "2025-05-11T02:20:49.428570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_classes=2, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids)  # (batch, seq_len, embed_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)     # (batch, seq_len, hidden_dim*2)\n",
    "\n",
    "        pooled, _ = torch.max(lstm_out, dim=1)  # (batch, hidden_dim*2)\n",
    "\n",
    "        output = self.dropout(pooled)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n"
   ],
   "id": "701a8eea894ac2d1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T02:24:12.021623Z",
     "start_time": "2025-05-11T02:24:10.596941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Detect vocab size from max token ID in dataset\n",
    "vocab_size = max(df[\"input_ids\"].explode()) + 1\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMClassifier(vocab_size=vocab_size).to(device)\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['target'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ],
   "id": "c562f8695bef0c76",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T02:24:32.401646Z",
     "start_time": "2025-05-11T02:24:28.553140Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tqdm",
   "id": "e28e3d1da5b64d98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (4.67.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-11T02:26:52.283552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 15\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "train_loss_list = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for input_ids, attention_mask, labels in loop:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        loop.set_postfix(avg_loss=avg_loss, accuracy=100 * correct / total)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_epoch_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in val_loader:\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    val_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"\\n Epoch {epoch+1} complete - Train Accuracy: {train_acc:.2f}% - Val Accuracy: {val_acc:.4f}\\n\")\n",
    "\n",
    "    # Track metrics\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc * 100)\n",
    "    train_loss_list.append(avg_epoch_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Best model saved!\")"
   ],
   "id": "54131665bc34e6e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 547/547 [14:15<00:00,  1.56s/it, accuracy=52.7, avg_loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 complete - Train Accuracy: 52.74% - Val Accuracy: 0.5944\n",
      "\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 547/547 [15:34<00:00,  1.71s/it, accuracy=57, avg_loss=0.676]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 2 complete - Train Accuracy: 56.99% - Val Accuracy: 0.6118\n",
      "\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:  19%|█▉        | 103/547 [16:38:30<71:44:16, 581.66s/it, accuracy=59.8, avg_loss=0.124]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 25\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     28\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PycharmProjects/AIPoweredCodeAssistant/venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/AIPoweredCodeAssistant/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_list, label=\"Train Accuracy\")\n",
    "plt.plot(val_acc_list, label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_list, label=\"Train Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "5839539f35c439bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T02:15:57.530182Z",
     "start_time": "2025-05-11T02:15:50.802925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df_test = pd.read_csv(\"datasets/security/test_preprocessed.csv\")\n",
    "df_test[\"input_ids\"] = df_test[\"input_ids\"].apply(ast.literal_eval)\n",
    "df_test[\"attention_mask\"] = df_test[\"attention_mask\"].apply(ast.literal_eval)\n",
    "df_test[\"target\"] = df_test[\"target\"].astype(int)\n"
   ],
   "id": "bac09a5993021f9b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T02:16:49.625672Z",
     "start_time": "2025-05-11T02:16:49.614536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset = CodeTokenDataset(df_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
   ],
   "id": "130c9a6bca8ba81c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T02:16:56.655115Z",
     "start_time": "2025-05-11T02:16:50.871135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(all_labels, all_preds))"
   ],
   "id": "3bfbad0a1838ba75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.67      1477\n",
      "           1       0.59      0.48      0.53      1255\n",
      "\n",
      "    accuracy                           0.61      2732\n",
      "   macro avg       0.61      0.60      0.60      2732\n",
      "weighted avg       0.61      0.61      0.60      2732\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b1132d549c639b09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
