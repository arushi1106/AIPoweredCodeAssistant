{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T16:19:00.572454Z",
     "start_time": "2025-06-21T16:18:58.537606Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install xgboost",
   "id": "c5c94d36a0e2b220",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in ./venv/lib/python3.10/site-packages (3.0.2)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from xgboost) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from xgboost) (1.15.2)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T16:19:19.143053Z",
     "start_time": "2025-06-21T16:19:07.875421Z"
    }
   },
   "cell_type": "code",
   "source": "!brew install libomp",
   "id": "156c79b8b53505d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m==>\u001B[0m \u001B[1mAuto-updating Homebrew...\u001B[0m\r\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\r\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\r\n",
      "\u001B[34m==>\u001B[0m \u001B[1mAuto-updated Homebrew!\u001B[0m\r\n",
      "Updated 2 taps (homebrew/core and homebrew/cask).\r\n",
      "\u001B[34m==>\u001B[0m \u001B[1mNew Formulae\u001B[0m\r\n",
      "arp-scan-rs     kbt             mermaid-cli     rnp             tiledb\r\n",
      "go-rice         lolcrab         ovsx            sherif\r\n",
      "\u001B[34m==>\u001B[0m \u001B[1mNew Casks\u001B[0m\r\n",
      "accordance@13              linqpad                    macsyzones\r\n",
      "\r\n",
      "You have \u001B[1m83\u001B[0m outdated formulae installed.\r\n",
      "\r\n",
      "\u001B[33mWarning:\u001B[0m libomp 20.1.7 is already installed and up-to-date.\r\n",
      "To reinstall 20.1.7, run:\r\n",
      "  brew reinstall libomp\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T16:20:04.731905Z",
     "start_time": "2025-06-21T16:20:02.488613Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tensorflow",
   "id": "6d6435f3fbf799d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.10/site-packages (2.16.2)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.3.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./venv/lib/python3.10/site-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.14.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.3.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from tensorflow) (25.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.25.8)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from tensorflow) (68.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.13.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.73.0)\r\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.16.2)\r\n",
      "Requirement already satisfied: keras>=3.0.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.10.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.26.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.8.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\r\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (14.0.0)\r\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.1.0)\r\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.19.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T18:08:36.365850Z",
     "start_time": "2025-06-21T18:08:36.293141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,  precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n"
   ],
   "id": "d5f963906f68e803",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T18:09:03.492122Z",
     "start_time": "2025-06-21T18:08:37.367252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load preprocessed CSVs\n",
    "train_df = pd.read_csv(\"datasets/security/train_preprocessed.csv\")\n",
    "valid_df = pd.read_csv(\"datasets/security/valid_preprocessed.csv\")\n",
    "test_df = pd.read_csv(\"datasets/security/test_preprocessed.csv\")\n",
    "\n",
    "MAX_LEN = 300\n",
    "\n",
    "def parse_sequence(x): return ast.literal_eval(x)[:MAX_LEN]\n",
    "X_train_seq = pad_sequences(train_df['input_ids'].apply(parse_sequence), maxlen=MAX_LEN)\n",
    "X_valid_seq = pad_sequences(valid_df['input_ids'].apply(parse_sequence), maxlen=MAX_LEN)\n",
    "X_test_seq = pad_sequences(test_df['input_ids'].apply(parse_sequence), maxlen=MAX_LEN)"
   ],
   "id": "f16bd5c39b5f2f72",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T18:09:03.702121Z",
     "start_time": "2025-06-21T18:09:03.499969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select top 10 tabular features\n",
    "features = [col for col in train_df.columns if col not in ['id', 'project', 'commit_id', 'tokens', 'func', 'input_ids', 'attention_mask', 'target']]\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "xgb.fit(train_df[features], train_df['target'].astype(int))\n",
    "top_features = pd.Series(xgb.feature_importances_, index=features).sort_values(ascending=False).head(10).index.tolist()\n",
    "\n",
    "X_train_tab = train_df[top_features]\n",
    "X_valid_tab = valid_df[top_features]\n",
    "X_test_tab = test_df[top_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_tab_scaled = scaler.fit_transform(X_train_tab)\n",
    "X_valid_tab_scaled = scaler.transform(X_valid_tab)\n",
    "X_test_tab_scaled = scaler.transform(X_test_tab)\n",
    "\n",
    "y_train = train_df['target'].astype(int)\n",
    "y_valid = valid_df['target'].astype(int)\n",
    "y_test = test_df['target'].astype(int)"
   ],
   "id": "5972d7eb66976066",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T18:09:03.767660Z",
     "start_time": "2025-06-21T18:09:03.740871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine train and valid\n",
    "X_seq_comb = np.concatenate([X_train_seq, X_valid_seq])\n",
    "X_tab_comb = np.concatenate([X_train_tab_scaled, X_valid_tab_scaled])\n",
    "y_comb = np.concatenate([y_train, y_valid])"
   ],
   "id": "31c4816f7d57f48a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T18:52:47.053867Z",
     "start_time": "2025-06-21T18:09:05.968375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CNN + Tabular Model\n",
    "vocab_size = max(np.max(X_seq_comb), np.max(X_test_seq)) + 1\n",
    "\n",
    "embed_dim = 64\n",
    "\n",
    "seq_input = Input(shape=(MAX_LEN,))\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=64)(seq_input)\n",
    "conv1 = Conv1D(256, 3, activation='relu', padding='same')(embed)\n",
    "conv2 = Conv1D(128, 5, activation='relu', padding='same')(conv1)\n",
    "pool = GlobalMaxPooling1D()(conv2)\n",
    "drop_seq = Dropout(0.5)(pool)\n",
    "\n",
    "tab_input = Input(shape=(X_tab_comb.shape[1],))\n",
    "\n",
    "concat = Concatenate()([drop_seq, tab_input])\n",
    "dense = Dense(64, activation='relu')(concat)\n",
    "drop = Dropout(0.4)(dense)\n",
    "output = Dense(1, activation='sigmoid')(drop)\n",
    "\n",
    "model = Model(inputs=[seq_input, tab_input], outputs=output)\n",
    "model.compile(optimizer=Adam(0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit([X_seq_comb, X_tab_comb], y_comb,\n",
    "          validation_data=([X_valid_seq, X_valid_tab_scaled], y_valid),\n",
    "          epochs=15,\n",
    "          batch_size=64)"
   ],
   "id": "27c7b378391a62f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m176s\u001B[0m 452ms/step - accuracy: 0.5310 - loss: 0.6957 - val_accuracy: 0.5835 - val_loss: 0.6582\n",
      "Epoch 2/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m170s\u001B[0m 442ms/step - accuracy: 0.5999 - loss: 0.6509 - val_accuracy: 0.8071 - val_loss: 0.4928\n",
      "Epoch 3/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m172s\u001B[0m 447ms/step - accuracy: 0.7768 - loss: 0.4671 - val_accuracy: 0.8946 - val_loss: 0.3036\n",
      "Epoch 4/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m175s\u001B[0m 455ms/step - accuracy: 0.8700 - loss: 0.2912 - val_accuracy: 0.9312 - val_loss: 0.1972\n",
      "Epoch 5/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m180s\u001B[0m 467ms/step - accuracy: 0.9126 - loss: 0.2020 - val_accuracy: 0.9462 - val_loss: 0.1444\n",
      "Epoch 6/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m263s\u001B[0m 685ms/step - accuracy: 0.9310 - loss: 0.1518 - val_accuracy: 0.9612 - val_loss: 0.1074\n",
      "Epoch 7/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m170s\u001B[0m 441ms/step - accuracy: 0.9426 - loss: 0.1230 - val_accuracy: 0.9568 - val_loss: 0.0910\n",
      "Epoch 8/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m164s\u001B[0m 426ms/step - accuracy: 0.9538 - loss: 0.0983 - val_accuracy: 0.9682 - val_loss: 0.0850\n",
      "Epoch 9/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m165s\u001B[0m 427ms/step - accuracy: 0.9599 - loss: 0.0831 - val_accuracy: 0.9674 - val_loss: 0.0730\n",
      "Epoch 10/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m163s\u001B[0m 424ms/step - accuracy: 0.9642 - loss: 0.0750 - val_accuracy: 0.9718 - val_loss: 0.0614\n",
      "Epoch 11/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m164s\u001B[0m 425ms/step - accuracy: 0.9688 - loss: 0.0651 - val_accuracy: 0.9762 - val_loss: 0.0539\n",
      "Epoch 12/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m164s\u001B[0m 426ms/step - accuracy: 0.9678 - loss: 0.0653 - val_accuracy: 0.9729 - val_loss: 0.0483\n",
      "Epoch 13/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m165s\u001B[0m 428ms/step - accuracy: 0.9712 - loss: 0.0552 - val_accuracy: 0.9769 - val_loss: 0.0451\n",
      "Epoch 14/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m165s\u001B[0m 428ms/step - accuracy: 0.9723 - loss: 0.0542 - val_accuracy: 0.9751 - val_loss: 0.0433\n",
      "Epoch 15/15\n",
      "\u001B[1m385/385\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m165s\u001B[0m 428ms/step - accuracy: 0.9723 - loss: 0.0543 - val_accuracy: 0.9773 - val_loss: 0.0401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b025930>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T18:52:52.475462Z",
     "start_time": "2025-06-21T18:52:50.249855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_proba = model.predict([X_test_seq, X_test_tab_scaled])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_thresh = thresholds[np.argmax(f1)]\n",
    "\n",
    "y_test_pred = (y_pred_proba > best_thresh).astype(int)\n",
    "\n",
    "print(\"Best threshold:\", best_thresh)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))"
   ],
   "id": "53ce0866703a89e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m86/86\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 22ms/step\n",
      "Best threshold: 5.5487316e-08\n",
      "Accuracy: 0.47291361639824303\n",
      "F1 Score: 0.6341463414634146\n",
      "Confusion Matrix:\n",
      " [[  44 1433]\n",
      " [   7 1248]]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:18:20.213598Z",
     "start_time": "2025-07-01T17:17:59.489003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ],
   "id": "a2ea95bbe4e97de0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:18:21.898603Z",
     "start_time": "2025-07-01T17:18:20.220563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Preprocessed Data\n",
    "train_df = pd.read_csv(\"datasets/security/train_preprocessed.csv\")\n",
    "valid_df = pd.read_csv(\"datasets/security/valid_preprocessed.csv\")\n",
    "test_df = pd.read_csv(\"datasets/security/test_preprocessed.csv\")"
   ],
   "id": "4e0c1e11ab29414f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:18:42.765285Z",
     "start_time": "2025-07-01T17:18:27.568295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_ids = []\n",
    "\n",
    "for df in [train_df, valid_df, test_df]:\n",
    "    ids_list = df['input_ids'].apply(eval).tolist()\n",
    "    flat_ids = [i for sublist in ids_list for i in sublist]\n",
    "    all_ids.extend(flat_ids)\n",
    "\n",
    "max_token_id = max(all_ids)\n",
    "print(f\"Max token ID in dataset: {max_token_id}\")"
   ],
   "id": "365bd7b1bf0c4a89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token ID in dataset: 477363\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:18:51.542436Z",
     "start_time": "2025-07-01T17:18:51.536918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "MAX_LEN = 100\n",
    "VOCAB_SIZE = max_token_id + 1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "be81144ddfa6fe47",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:18:52.409426Z",
     "start_time": "2025-07-01T17:18:52.403358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset Class\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.X = df['input_ids'].apply(eval).tolist()\n",
    "        self.y = df['target'].astype(int).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.X[idx][:MAX_LEN]\n",
    "        tokens += [0] * (MAX_LEN - len(tokens))  # Padding\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.float32)"
   ],
   "id": "6213103fb7c02e32",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:19:07.555177Z",
     "start_time": "2025-07-01T17:18:53.141442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Loaders\n",
    "train_dataset = CodeDataset(train_df)\n",
    "valid_dataset = CodeDataset(valid_df)\n",
    "test_dataset = CodeDataset(test_df)"
   ],
   "id": "47a020e483e492be",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:19:07.564134Z",
     "start_time": "2025-07-01T17:19:07.560478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ],
   "id": "8231209e45f1e1df",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:19:07.593523Z",
     "start_time": "2025-07-01T17:19:07.587871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CNN Model\n",
    "class CodeCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_filters=128, kernel_size=5):\n",
    "        super(CodeCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, num_filters, kernel_size)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(num_filters, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)  # (B, embed_dim, seq_len)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)"
   ],
   "id": "12112a5884880999",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:19:12.625765Z",
     "start_time": "2025-07-01T17:19:07.617846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Setup\n",
    "model = CodeCNN(VOCAB_SIZE).to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "25a6db0669e028c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:19:23.915230Z",
     "start_time": "2025-07-01T17:19:12.667229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(train_dataset))  # Should be reasonable (e.g., thousands)\n",
    "all_lengths = [len(eval(x)) for x in train_df['input_ids']]\n",
    "print(f\"Max token length: {max(all_lengths)}\")  # Should be <500 ideally\n"
   ],
   "id": "1d85de9aaa2b71a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21854\n",
      "Max token length: 512\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:41:58.143682Z",
     "start_time": "2025-07-01T17:20:01.090185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed, Total Loss: {total_loss/len(train_loader):.4f}\")\n"
   ],
   "id": "d534b05fa96b63d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/683, Loss: 0.7160\n",
      "Batch 50/683, Loss: 0.7959\n",
      "Batch 100/683, Loss: 0.6495\n",
      "Batch 150/683, Loss: 0.6825\n",
      "Batch 200/683, Loss: 0.5893\n",
      "Batch 250/683, Loss: 0.6934\n",
      "Batch 300/683, Loss: 0.7086\n",
      "Batch 350/683, Loss: 0.6218\n",
      "Batch 400/683, Loss: 0.7002\n",
      "Batch 450/683, Loss: 0.7199\n",
      "Batch 500/683, Loss: 0.7291\n",
      "Batch 550/683, Loss: 0.7679\n",
      "Batch 600/683, Loss: 0.6093\n",
      "Batch 650/683, Loss: 0.7211\n",
      "Epoch 1 completed, Total Loss: 0.6808\n",
      "Batch 0/683, Loss: 0.5981\n",
      "Batch 50/683, Loss: 0.5883\n",
      "Batch 100/683, Loss: 0.5894\n",
      "Batch 150/683, Loss: 0.6004\n",
      "Batch 200/683, Loss: 0.5615\n",
      "Batch 250/683, Loss: 0.6563\n",
      "Batch 300/683, Loss: 0.6006\n",
      "Batch 350/683, Loss: 0.5571\n",
      "Batch 400/683, Loss: 0.5728\n",
      "Batch 450/683, Loss: 0.5977\n",
      "Batch 500/683, Loss: 0.6736\n",
      "Batch 550/683, Loss: 0.5012\n",
      "Batch 600/683, Loss: 0.5444\n",
      "Batch 650/683, Loss: 0.6480\n",
      "Epoch 2 completed, Total Loss: 0.5794\n",
      "Batch 0/683, Loss: 0.3624\n",
      "Batch 50/683, Loss: 0.3812\n",
      "Batch 100/683, Loss: 0.2627\n",
      "Batch 150/683, Loss: 0.2916\n",
      "Batch 200/683, Loss: 0.4063\n",
      "Batch 250/683, Loss: 0.3499\n",
      "Batch 300/683, Loss: 0.4974\n",
      "Batch 350/683, Loss: 0.3689\n",
      "Batch 400/683, Loss: 0.3960\n",
      "Batch 450/683, Loss: 0.4720\n",
      "Batch 500/683, Loss: 0.7287\n",
      "Batch 550/683, Loss: 0.5115\n",
      "Batch 600/683, Loss: 0.4446\n",
      "Batch 650/683, Loss: 0.3769\n",
      "Epoch 3 completed, Total Loss: 0.4332\n",
      "Batch 0/683, Loss: 0.1927\n",
      "Batch 50/683, Loss: 0.3046\n",
      "Batch 100/683, Loss: 0.2382\n",
      "Batch 150/683, Loss: 0.3062\n",
      "Batch 200/683, Loss: 0.1870\n",
      "Batch 250/683, Loss: 0.4198\n",
      "Batch 300/683, Loss: 0.4978\n",
      "Batch 350/683, Loss: 0.2493\n",
      "Batch 400/683, Loss: 0.3780\n",
      "Batch 450/683, Loss: 0.3843\n",
      "Batch 500/683, Loss: 0.2311\n",
      "Batch 550/683, Loss: 0.2281\n",
      "Batch 600/683, Loss: 0.3109\n",
      "Batch 650/683, Loss: 0.4462\n",
      "Epoch 4 completed, Total Loss: 0.2932\n",
      "Batch 0/683, Loss: 0.1144\n",
      "Batch 50/683, Loss: 0.1514\n",
      "Batch 100/683, Loss: 0.0690\n",
      "Batch 150/683, Loss: 0.2122\n",
      "Batch 200/683, Loss: 0.2225\n",
      "Batch 250/683, Loss: 0.2707\n",
      "Batch 300/683, Loss: 0.1051\n",
      "Batch 350/683, Loss: 0.1520\n",
      "Batch 400/683, Loss: 0.1806\n",
      "Batch 450/683, Loss: 0.2697\n",
      "Batch 500/683, Loss: 0.2848\n",
      "Batch 550/683, Loss: 0.2571\n",
      "Batch 600/683, Loss: 0.2818\n",
      "Batch 650/683, Loss: 0.3205\n",
      "Epoch 5 completed, Total Loss: 0.2163\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:42:14.182490Z",
     "start_time": "2025-07-01T17:42:13.562728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation on Test Set\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        outputs = model(X_batch).squeeze().cpu().numpy()\n",
    "        preds = (outputs > 0.5).astype(int)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(preds)"
   ],
   "id": "986fa0d20dbea393",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:42:15.698173Z",
     "start_time": "2025-07-01T17:42:15.606459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Classification Report\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"\\nTest Set Performance:\\n\")\n",
    "print(report)"
   ],
   "id": "8677c63c80d442d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Performance:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.69      0.62      1477\n",
      "         1.0       0.51      0.38      0.44      1255\n",
      "\n",
      "    accuracy                           0.55      2732\n",
      "   macro avg       0.54      0.54      0.53      2732\n",
      "weighted avg       0.54      0.55      0.54      2732\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b6f79e03ba677f3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
