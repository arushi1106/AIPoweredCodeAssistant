{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:34:39.177206Z",
     "start_time": "2025-05-12T23:34:39.110431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast"
   ],
   "id": "830edce7a9f9f142",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:34:59.420893Z",
     "start_time": "2025-05-12T23:34:40.290205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load Preprocessed Data\n",
    "df_train = pd.read_csv(\"datasets/security/train_preprocessed.csv\")\n",
    "df_valid = pd.read_csv(\"datasets/security/valid_preprocessed.csv\")\n",
    "df_test = pd.read_csv(\"datasets/security/test_preprocessed.csv\")\n",
    "\n",
    "# Evaluate input_ids (convert string lists to actual lists)\n",
    "df_train[\"input_ids\"] = df_train[\"input_ids\"].apply(eval)\n",
    "df_valid[\"input_ids\"] = df_valid[\"input_ids\"].apply(eval)\n",
    "df_test[\"input_ids\"] = df_test[\"input_ids\"].apply(eval)\n",
    "\n",
    "# Get max token ID\n",
    "vocab_size = max(\n",
    "    max(df_train[\"input_ids\"].explode()),\n",
    "    max(df_valid[\"input_ids\"].explode()),\n",
    "    max(df_test[\"input_ids\"].explode())\n",
    ") + 1\n",
    "\n",
    "print(f\"Vocab size determined: {vocab_size}\")"
   ],
   "id": "14a20d1fc03420b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size determined: 477364\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:34:59.519540Z",
     "start_time": "2025-05-12T23:34:59.505537Z"
    }
   },
   "cell_type": "code",
   "source": "print(df_train.columns.tolist())",
   "id": "bc64f6a0e220f67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'func', 'target', 'project', 'commit_id', 'tokens', 'input_ids', 'attention_mask', 'func_length', 'num_loops', 'has_eval', 'has_system', 'num_if', 'num_return', 'uses_pointer', 'uses_buffer', 'is_short_func']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:35:22.646433Z",
     "start_time": "2025-05-12T23:35:22.623148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SecurityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, vocab_size):\n",
    "        self.input_ids = df[\"input_ids\"].tolist()\n",
    "        self.attention_masks = df[\"attention_mask\"].tolist()\n",
    "        self.meta_features = df[[\"func_length\", \"num_loops\", \"num_if\", \"num_return\",\n",
    "                                 \"has_eval\", \"has_system\", \"uses_pointer\", \"uses_buffer\"]].values\n",
    "        self.labels = df[\"target\"].values\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = [min(tid, self.vocab_size - 1) for tid in self.input_ids[idx]]\n",
    "        mask = self.attention_masks[idx]\n",
    "        if isinstance(mask, str):\n",
    "            mask = ast.literal_eval(mask)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(ids, dtype=torch.long),\n",
    "            torch.tensor(mask, dtype=torch.long),\n",
    "            torch.tensor(self.meta_features[idx], dtype=torch.float),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "id": "5df6c1cb4dd4f4b6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:35:24.695289Z",
     "start_time": "2025-05-12T23:35:24.540321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = SecurityDataset(df_train, vocab_size)\n",
    "val_dataset = SecurityDataset(df_valid, vocab_size)\n",
    "test_dataset = SecurityDataset(df_test, vocab_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ],
   "id": "5b3c9e1089d330e4",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:35:25.685032Z",
     "start_time": "2025-05-12T23:35:25.675443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. CNN + Metadata Model\n",
    "class CNNMetadataFusion(nn.Module):\n",
    "    def __init__(self, vocab_size, meta_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.meta_dense = nn.Linear(meta_dim, 32)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 + 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, meta_features):\n",
    "        x = self.embedding(input_ids).permute(0, 2, 1)\n",
    "        x = self.pool(torch.relu(self.conv(x))).squeeze(2)\n",
    "        m = torch.relu(self.meta_dense(meta_features))\n",
    "        combined = torch.cat([x, m], dim=1)\n",
    "        return self.classifier(combined)\n"
   ],
   "id": "bd1f8403f2698d7",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:35:28.192434Z",
     "start_time": "2025-05-12T23:35:26.626823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = max(df_train[\"input_ids\"].explode()) + 1\n",
    "model = CNNMetadataFusion(vocab_size, meta_dim=8).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ],
   "id": "f281027cc67176e9",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-12T23:35:35.829956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Training Loop\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/15\")\n",
    "\n",
    "    for input_ids, attention_mask, meta, labels in loop:\n",
    "        input_ids, attention_mask, meta, labels = (\n",
    "            input_ids.to(device), attention_mask.to(device), meta.to(device), labels.to(device)\n",
    "        )\n",
    "        outputs = model(input_ids, attention_mask, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_postfix(avg_loss=total_loss/(total//32 + 1), accuracy=100 * correct / total)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, meta, labels in val_loader:\n",
    "            input_ids, attention_mask, meta = input_ids.to(device), attention_mask.to(device), meta.to(device)\n",
    "            outputs = model(input_ids, attention_mask, meta)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(labels.tolist())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_accs.append(val_acc)\n",
    "    print(f\"\\nEpoch {epoch+1} complete - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\\n\")"
   ],
   "id": "c2cea9592d9c03c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|          | 0/683 [00:00<?, ?it/s]/var/folders/7b/fjw7n14j4v7dpzttsjwh2mc80000gn/T/ipykernel_1237/695653159.py:20: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "Epoch 1/15:   2%|â–         | 17/683 [00:07<05:00,  2.22it/s, accuracy=53.7, avg_loss=0.725]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T23:05:23.871685Z",
     "start_time": "2025-05-10T23:05:23.864165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. Evaluation on Test Set\n",
    "model.eval()\n",
    "test_preds, test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, meta, labels in test_loader:\n",
    "        input_ids, attention_mask, meta = input_ids.to(device), attention_mask.to(device), meta.to(device)\n",
    "        outputs = model(input_ids, attention_mask, meta)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        test_preds.extend(preds)\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds))"
   ],
   "id": "21d3796ff332329a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T02:20:49.473396Z",
     "start_time": "2025-05-11T02:20:49.428570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. Accuracy Graph\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "701a8eea894ac2d1",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
