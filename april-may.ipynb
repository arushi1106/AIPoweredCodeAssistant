{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T22:57:03.423860Z",
     "start_time": "2025-05-12T22:57:02.063417Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:57:08.135217Z",
     "start_time": "2025-05-12T22:57:08.130512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create dataset directory\n",
    "os.makedirs(\"datasets/security\", exist_ok=True)\n",
    "print(\"Created folder: datasets/security\")\n"
   ],
   "id": "ef8dc639849ff828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: datasets/security\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:59:01.288368Z",
     "start_time": "2025-05-12T22:58:57.759418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "raw_dataset = load_dataset(\"code_x_glue_cc_defect_detection\")\n",
    "\n",
    "# Check the available splits\n",
    "print(raw_dataset)\n",
    "\n",
    "# Show a few samples\n",
    "raw_dataset[\"train\"][0]"
   ],
   "id": "72ccba28245ee7ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'func', 'target', 'project', 'commit_id'],\n",
      "        num_rows: 21854\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'func', 'target', 'project', 'commit_id'],\n",
      "        num_rows: 2732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'func', 'target', 'project', 'commit_id'],\n",
      "        num_rows: 2732\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'func': 'static av_cold int vdadec_init(AVCodecContext *avctx)\\n\\n{\\n\\n    VDADecoderContext *ctx = avctx->priv_data;\\n\\n    struct vda_context *vda_ctx = &ctx->vda_ctx;\\n\\n    OSStatus status;\\n\\n    int ret;\\n\\n\\n\\n    ctx->h264_initialized = 0;\\n\\n\\n\\n    /* init pix_fmts of codec */\\n\\n    if (!ff_h264_vda_decoder.pix_fmts) {\\n\\n        if (kCFCoreFoundationVersionNumber < kCFCoreFoundationVersionNumber10_7)\\n\\n            ff_h264_vda_decoder.pix_fmts = vda_pixfmts_prior_10_7;\\n\\n        else\\n\\n            ff_h264_vda_decoder.pix_fmts = vda_pixfmts;\\n\\n    }\\n\\n\\n\\n    /* init vda */\\n\\n    memset(vda_ctx, 0, sizeof(struct vda_context));\\n\\n    vda_ctx->width = avctx->width;\\n\\n    vda_ctx->height = avctx->height;\\n\\n    vda_ctx->format = \\'avc1\\';\\n\\n    vda_ctx->use_sync_decoding = 1;\\n\\n    vda_ctx->use_ref_buffer = 1;\\n\\n    ctx->pix_fmt = avctx->get_format(avctx, avctx->codec->pix_fmts);\\n\\n    switch (ctx->pix_fmt) {\\n\\n    case AV_PIX_FMT_UYVY422:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'2vuy\\';\\n\\n        break;\\n\\n    case AV_PIX_FMT_YUYV422:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'yuvs\\';\\n\\n        break;\\n\\n    case AV_PIX_FMT_NV12:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'420v\\';\\n\\n        break;\\n\\n    case AV_PIX_FMT_YUV420P:\\n\\n        vda_ctx->cv_pix_fmt_type = \\'y420\\';\\n\\n        break;\\n\\n    default:\\n\\n        av_log(avctx, AV_LOG_ERROR, \"Unsupported pixel format: %d\\\\n\", avctx->pix_fmt);\\n\\n        goto failed;\\n\\n    }\\n\\n    status = ff_vda_create_decoder(vda_ctx,\\n\\n                                   avctx->extradata, avctx->extradata_size);\\n\\n    if (status != kVDADecoderNoErr) {\\n\\n        av_log(avctx, AV_LOG_ERROR,\\n\\n                \"Failed to init VDA decoder: %d.\\\\n\", status);\\n\\n        goto failed;\\n\\n    }\\n\\n    avctx->hwaccel_context = vda_ctx;\\n\\n\\n\\n    /* changes callback functions */\\n\\n    avctx->get_format = get_format;\\n\\n    avctx->get_buffer2 = get_buffer2;\\n\\n#if FF_API_GET_BUFFER\\n\\n    // force the old get_buffer to be empty\\n\\n    avctx->get_buffer = NULL;\\n\\n#endif\\n\\n\\n\\n    /* init H.264 decoder */\\n\\n    ret = ff_h264_decoder.init(avctx);\\n\\n    if (ret < 0) {\\n\\n        av_log(avctx, AV_LOG_ERROR, \"Failed to open H.264 decoder.\\\\n\");\\n\\n        goto failed;\\n\\n    }\\n\\n    ctx->h264_initialized = 1;\\n\\n\\n\\n    return 0;\\n\\n\\n\\nfailed:\\n\\n    vdadec_close(avctx);\\n\\n    return -1;\\n\\n}\\n',\n",
       " 'target': False,\n",
       " 'project': 'FFmpeg',\n",
       " 'commit_id': '973b1a6b9070e2bf17d17568cbaf4043ce931f51'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:59:11.546903Z",
     "start_time": "2025-05-12T22:59:08.112049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to DataFrames\n",
    "df_train = pd.DataFrame(raw_dataset[\"train\"])\n",
    "df_valid = pd.DataFrame(raw_dataset[\"validation\"])\n",
    "df_test = pd.DataFrame(raw_dataset[\"test\"])\n",
    "\n",
    "# Basic Cleaning and Save Raw\n",
    "df_train.to_csv(\"datasets/security/train.csv\", index=False)\n",
    "df_valid.to_csv(\"datasets/security/valid.csv\", index=False)\n",
    "df_test.to_csv(\"datasets/security/test.csv\", index=False)\n",
    "\n",
    "print(\"Dataset saved as CSV files.\")\n"
   ],
   "id": "30d2830998c9690f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as CSV files.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:59:32.258505Z",
     "start_time": "2025-05-12T22:59:31.646428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer (simple whitespace split for CNN or embedding)\n",
    "def tokenize_func(func_str):\n",
    "    return func_str.split()\n",
    "\n",
    "df_train[\"tokens\"] = df_train[\"func\"].apply(tokenize_func)\n",
    "df_valid[\"tokens\"] = df_valid[\"func\"].apply(tokenize_func)\n",
    "df_test[\"tokens\"] = df_test[\"func\"].apply(tokenize_func)\n",
    "\n",
    "# Dummy vocab index for illustration\n",
    "from collections import defaultdict\n",
    "vocab = defaultdict(lambda: len(vocab))\n",
    "vocab[\"<PAD>\"]  # make sure PAD token is 0"
   ],
   "id": "50e33374d6d2ca37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:00:15.911251Z",
     "start_time": "2025-05-12T23:00:13.127550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode(tokens):\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "df_train[\"input_ids\"] = df_train[\"tokens\"].apply(encode)\n",
    "df_valid[\"input_ids\"] = df_valid[\"tokens\"].apply(encode)\n",
    "df_test[\"input_ids\"] = df_test[\"tokens\"].apply(encode)"
   ],
   "id": "320f7b9a3d8a36cc",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:01:15.191472Z",
     "start_time": "2025-05-12T23:01:12.485868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Truncate / pad input_ids to 512\n",
    "MAX_LEN = 512\n",
    "def pad(seq):\n",
    "    seq = seq[:MAX_LEN]\n",
    "    return seq + [0]*(MAX_LEN - len(seq))\n",
    "\n",
    "df_train[\"input_ids\"] = df_train[\"input_ids\"].apply(pad)\n",
    "df_valid[\"input_ids\"] = df_valid[\"input_ids\"].apply(pad)\n",
    "df_test[\"input_ids\"] = df_test[\"input_ids\"].apply(pad)\n",
    "\n",
    "df_train[\"attention_mask\"] = df_train[\"input_ids\"].apply(lambda x: [1 if i > 0 else 0 for i in x])\n",
    "df_valid[\"attention_mask\"] = df_valid[\"input_ids\"].apply(lambda x: [1 if i > 0 else 0 for i in x])\n",
    "df_test[\"attention_mask\"] = df_test[\"input_ids\"].apply(lambda x: [1 if i > 0 else 0 for i in x])"
   ],
   "id": "1eaa5c833541db2f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:01:27.339675Z",
     "start_time": "2025-05-12T23:01:27.316673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Engineering\n",
    "def add_features(df):\n",
    "    df[\"func_length\"] = df[\"tokens\"].apply(len)\n",
    "    df[\"num_loops\"] = df[\"func\"].apply(lambda x: x.count(\"for\") + x.count(\"while\"))\n",
    "    df[\"has_eval\"] = df[\"func\"].apply(lambda x: int(\"eval\" in x))\n",
    "    df[\"has_system\"] = df[\"func\"].apply(lambda x: int(\"system\" in x or \"exec\" in x))\n",
    "    df[\"num_if\"] = df[\"func\"].apply(lambda x: x.count(\"if\"))\n",
    "    df[\"num_return\"] = df[\"func\"].apply(lambda x: x.count(\"return\"))\n",
    "    df[\"uses_pointer\"] = df[\"func\"].apply(lambda x: int(\"*\" in x))\n",
    "    df[\"uses_buffer\"] = df[\"func\"].apply(lambda x: int(\"buffer\" in x or \"memcpy\" in x or \"strcpy\" in x))\n",
    "    return df"
   ],
   "id": "63f4f05503cc420c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:01:43.405853Z",
     "start_time": "2025-05-12T23:01:42.514537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply feature engineering\n",
    "df_train = add_features(df_train)\n",
    "df_valid = add_features(df_valid)\n",
    "df_test = add_features(df_test)"
   ],
   "id": "1a7b5af8075a00cc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:02:03.456638Z",
     "start_time": "2025-05-12T23:01:53.605265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pre_dir = \"datasets/security\"\n",
    "df_train.to_csv(f\"{pre_dir}/train_preprocessed.csv\", index=False)\n",
    "df_valid.to_csv(f\"{pre_dir}/valid_preprocessed.csv\", index=False)\n",
    "df_test.to_csv(f\"{pre_dir}/test_preprocessed.csv\", index=False)"
   ],
   "id": "2a4210fd7c5bfe86",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:02:13.015201Z",
     "start_time": "2025-05-12T23:02:12.998824Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Preprocessing complete. Data saved.\")",
   "id": "547ca05e23fc8211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Data saved.\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
